<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.6"/>
<title>Superpixel Benchmark: Datasets</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/javascript">
  $(document).ready(function() { searchBox.OnSelectItem(0); });
</script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td style="padding-left: 0.5em;">
   <div id="projectname">Superpixel Benchmark
   </div>
   <div id="projectbrief">Superpixel benchmark, tools and algorithms.</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.6 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
  <div id="navrow1" class="tabs">
    <ul class="tablist">
      <li><a href="index.html"><span>Main&#160;Page</span></a></li>
      <li class="current"><a href="pages.html"><span>Related&#160;Pages</span></a></li>
      <li><a href="annotated.html"><span>Classes</span></a></li>
      <li><a href="files.html"><span>Files</span></a></li>
      <li>
        <div id="MSearchBox" class="MSearchBoxInactive">
        <span class="left">
          <img id="MSearchSelect" src="search/mag_sel.png"
               onmouseover="return searchBox.OnSearchSelectShow()"
               onmouseout="return searchBox.OnSearchSelectHide()"
               alt=""/>
          <input type="text" id="MSearchField" value="Search" accesskey="S"
               onfocus="searchBox.OnSearchFieldFocus(true)" 
               onblur="searchBox.OnSearchFieldFocus(false)" 
               onkeyup="searchBox.OnSearchFieldChange(event)"/>
          </span><span class="right">
            <a id="MSearchClose" href="javascript:searchBox.CloseResultsWindow()"><img id="MSearchCloseImg" border="0" src="search/close.png" alt=""/></a>
          </span>
        </div>
      </li>
    </ul>
  </div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
<a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(0)"><span class="SelectionMark">&#160;</span>All</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(1)"><span class="SelectionMark">&#160;</span>Classes</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(2)"><span class="SelectionMark">&#160;</span>Files</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(3)"><span class="SelectionMark">&#160;</span>Functions</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(4)"><span class="SelectionMark">&#160;</span>Variables</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(5)"><span class="SelectionMark">&#160;</span>Friends</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(6)"><span class="SelectionMark">&#160;</span>Macros</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(7)"><span class="SelectionMark">&#160;</span>Pages</a></div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

</div><!-- top -->
<div class="header">
  <div class="headertitle">
<div class="title">Datasets </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><p><a class="el" href="classEvaluation.html" title="Provides measures to evaluate (over-) segmentations. ">Evaluation</a> was based on five datasets; these are briefly introduced in the following. The pre-processing, formating and conversion tools used are detailed afterwards.</p>
<ul>
<li>[BSDS500](bsds500)</li>
<li>[NYUV2](nyuv2)</li>
<li>[SBD](sbd)</li>
<li>[SUNRGBD](sunrgbd)</li>
<li>[Fash](fash)</li>
</ul>
<p><b>Downloads of the pre-processed datasets will be made available (if possible) here: <a href="https://github.com/davidstutz/superpixel-benchmark-data">davidstutz/superpixel-benchmark-data</a></b></p>
<p>Additional details can be found in [1]. </p>
<pre class="fragment">[1] D. Stutz, A. Hermans, B. Leibe.
    Superpixels: An Evaluation of the State-of-the-Art.
    Computing Research Repository, abs/1612.01601.
</pre><div class="image">
<img src="DATASETS.png?raw=true"  alt="Dataset overview." title="Dataset overview."/>
</div>
<h2>BSDS500</h2>
<p>The Berkeley Segmentation Dataset 500 [2] was the first dataset used for evaluating superpixel algorithms. It consists of 500 images, each with 5 different ground truth segmentations of high quality, divided into a training set of 200 images, a validation set of 100 images and a test set og 200 images. For parameter optimization, the validation set was used. </p>
<pre class="fragment">[2] P. Arbelaez, M. Maire, C. Fowlkes, J. Malik.
    Contour detection and hierarchical image segmentation.
    IEEE Transactions on Pattern Analysis and Machine Intelligence 33 (5) (2011) 898-916.
</pre><p>The ground truth was used as provided, however, converted from <code>.mat</code> format to <code>.csv</code> format. The converted dataset is available in the data repository: <a href="https://github.com/davidstutz/superpixel-evaluation-data">davidstutz/superpixel-evaluation-data</a>.</p>
<p>In order to manually convert the BSDS500 dataset, use <code>lib_tools/bsds500_convert_script.m</code>:</p>
<ol type="1">
<li>Download the BSDS500 dataset from <a href="https://www2.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/resources.html">here</a>.</li>
<li>Extract the BSR/BSDS500/data folder into data/BSDS500 (overwriting the provided examples in data/BSDS500/images). Also note that afterwards there are three folders: <code>groundTruth</code>, <code>csv_groundTruth</code> and <code>images</code>.</li>
<li>Adapt the path to the directory below, i.e. set <code>BSDS500_DIR</code> correctly.</li>
<li>Run the script. Note that this may take some time and memory.</li>
</ol>
<h2>NYUV2</h2>
<p>The NYU Depth Dataset V2 [3] includes 1449 images with pre-processed depth. Semantic ground truth segmentations with instance labels are provided. Following Ren and Bo [4], the ground truth has been pre-processed to remove small unlabeled segments. 199 images were randomly chosen to represent the validation set and 399 images where randomly chosen for testing. </p>
<pre class="fragment">[3] N. Silberman, D. Hoiem, P. Kohli, R. Fergus.
    Indoor segmentation and support inference from RGBD images.
    European Conference on Computer Vision, 2012, pp. 746–760.
[4] X. Ren, L. Bo.
    Discriminatively trained sparse code gradients for contour detection.
    Neural Information Processing Systems, 2012, pp. 593–601.
</pre><p>The ground truth was converted to <code>.csv</code> files after thinning unlabeled regions. The converted dataset (i.e. the corresponding subsets used for validation and testing) is available in the data repository: <a href="https://github.com/davidstutz/superpixel-evaluation-data">davidstutz/superpixel-evaluation-data</a>.</p>
<p>In order to manually convert the NYUV2 dataset and extract the used validation and testing subsets, use <code>lib_tools/nyuv2_convert.script.m</code>:</p>
<ol type="1">
<li>Download the dataset from <a href="http://cs.nyu.edu/~silberman/datasets/nyu_depth_v2.html">here</a>. Make sure that the downloaded file is nyu_depth_v2_labeled.mat.</li>
<li>Put the file in <code>data/NYUV2</code>.</li>
<li>Make sure that <code>data/NYUV2 contains</code> contains <code>nyuv2_test_subset.txt</code>, <code>nyuv2_train_subset.txt</code>, <code>nyuv2_test.txt</code> and <code>nyuv2_train.txt</code>.</li>
<li>Adapt the variables below, in particular <code>NYUV2_DIR</code> to point to the <code>data/NYUV2</code> directory.</li>
<li>Run the script. Note that this may take some time and memory.</li>
</ol>
<h2>SBD</h2>
<p>The Stanford Background Dataset [5] combines 715 images from several datasets (see [1] for details). The images are of varying size, quality and scenes. The semantic ground truth segmentations provided needed to be pre-processed in order to guarantee connected components. Validation and testing sets of size 238 and 477, respectively, were chosen at random. </p>
<pre class="fragment">[5] S. Gould, R. Fulton, D. Koller.
    Decomposing a scene into geometric and semantically consistent regions.
    International Conference on Computer Vision, 2009, pp. 1–8.
</pre><p>The ground truth was converted to <code>.csv</code> files. The converted dataset (and the corresponding division into validation and testing) is available in the data repository: <a href="https://github.com/davidstutz/superpixel-evaluation-data">davidstutz/superpixel-evaluation-data</a>.</p>
<p>To manually convert the SBD and select validation and testing images, follow <code>lib_tools/sbd_convert_script.m</code>:</p>
<ol type="1">
<li>Download the Stanford Background Dataset from <a href="http://dags.stanford.edu/projects/scenedataset.html">here</a>.</li>
<li>Extract the dataset such that <code>data/SBD</code> contains two folders: <code>images</code> and <code>labels</code>.</li>
<li>Make sure that <code>data/SBD</code> contains <code>sbd_test.txt</code> and <code>sbd_train.txt</code>.</li>
<li>Adapt the variable <code>SBD_DIR</code> below to match the path to <code>data/SBD</code>.</li>
<li>Run the script. Note that this may take some time and memory.</li>
</ol>
<h2>SUNRGBD</h2>
<p>The SUNRGBD dataset [6] contains 10335 images including pre-processed depth. Semantic ground truth segmentations are provided and need to be pre-processed similar to the NYUV2 dataset. Validation set and testing set of size 200 and 400, respectively, were chosen at random. Images that are also included in the NYUV2 dataset were ignored. </p>
<pre class="fragment">[6] S. Song, S. P. Lichtenberg, J. Xiao.
    SUN RGB-D: A RGB-D scene understanding benchmark suite.
    IEEE Conference on Computer Vision and Pattern Recognition, 2015, pp. 567–576.
</pre><p>The ground truth was converted to <code>.csv</code> files. The converted dataset (including the validation/test split) is available in the data repository: <a href="https://github.com/davidstutz/superpixel-evaluation-data">davidstutz/superpixel-evaluation-data</a>.</p>
<p>To manually convert the dataset, follow <code>lib_tools/sunrgbd_convert_script.m</code>:</p>
<ol type="1">
<li>Download the SUNRGBD dataset from <a href="http://rgbd.cs.princeton.edu/">here</a>.</li>
<li>Make sure to download both the SUNRGBD V1 dataset and the SUNRGBDtoolbox containing the annotations.</li>
<li>From the SUNRGBDtoolbox extract <code>SUNRGBD2dseg.mat</code> and <code>SUNRGBDMeta.mat</code> to <code>data/SUNRGBD</code>.</li>
<li>From the SUNRGBD V1 dataset extract all files into <code>data/SUNRGBD</code>; note that this may take quite some time! It might be wise to extract the contained directories (xtion, realsense, kv1, kv2) separately as this may take some time!</li>
<li>Run the script. Note that this may take some time and memory.</li>
</ol>
<h2>Fash</h2>
<p>The Fashionista dataset [7] contains 685 images with semantic ground truth segmentations. The ground truth segmentations were pre-processed to ensure connected segments. Validation set and training set of size 222 and 463, respectively, were chosen at random. </p>
<pre class="fragment">[7] K. Yamaguchi, K. M. H, L. E. Ortiz, T. L. Berg.
    Parsing clothing in fashion photographs.
    IEEE Conference on Computer Vision and Pattern Recognition, 2012, pp. 3570–3577.
</pre><p>The ground truth was converted to <code>.csv</code> files. The converted dataset is available in the data repository: <a href="https://github.com/davidstutz/superpixel-evaluation-data">davidstutz/superpixel-evaluation-data</a>.</p>
<p>In order to manually convert the dataset, follow the steps in <code>lib_tools/fash_convert_script.m</code>:</p>
<ol type="1">
<li>Download the Fashionista dataset from <a href="http://vision.is.tohoku.ac.jp/~kyamagu/research/clothing_parsing/">here</a>.</li>
<li>Extract fashionista_v0.2.1.mat into data/Fash.</li>
<li>Adapt the below variables to match the path where <code>data/Fash</code> can be found.</li>
<li>Run the script. Note that this may take some time and memory.</li>
</ol>
<h2>Licenses</h2>
<p>License details can be found in the corresponding <code>README.md</code> files included in the corresponding subdirectories. We also include the original links below:</p>
<table class="doxtable">
<tr>
<th>Dataset </th><th>Link  </th></tr>
<tr>
<td>BSDS500 </td><td><a href="https://www2.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/resources.html">Web</a> </td></tr>
<tr>
<td>NYUV2 </td><td><a href="http://cs.nyu.edu/~silberman/datasets/nyu_depth_v2.html">Web</a> </td></tr>
<tr>
<td>SBD </td><td><a href="http://dags.stanford.edu/projects/scenedataset.html">Web</a> </td></tr>
<tr>
<td>SUNRGBD </td><td><a href="http://rgbd.cs.princeton.edu/">Web</a> </td></tr>
<tr>
<td>Fash </td><td><a href="http://vision.is.tohoku.ac.jp/~kyamagu/research/clothing_parsing/">Web</a> </td></tr>
</table>
</div></div><!-- contents -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated on Mon Dec 19 2016 11:48:33 for Superpixel Benchmark by &#160;<a href="http://www.doxygen.org/index.html">
<img class="footer" src="doxygen.png" alt="doxygen"/>
</a> 1.8.6
</small></address>
</body>
</html>
